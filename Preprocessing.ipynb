{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Statements Page Classification - Data Preprocessing\n",
    "\n",
    "## Overview\n",
    "This notebook preprocesses the financial statement pages for training three different computer vision models:\n",
    "\n",
    "1. **ResNet50** - Classic CNN with transfer learning\n",
    "2. **EfficientNet-B2** - Modern efficient architecture\n",
    "3. **Vision Transformer (ViT)** - Transformer-based approach\n",
    "\n",
    "### Dataset Statistics (from EDA):\n",
    "- Total PDF documents: 30\n",
    "- Total pages: 1,179\n",
    "- Average dimensions: 1277 x 1692 pixels\n",
    "- 5 Target Classes: Independent Auditor's Report, Financial Sheets, Notes (Tabular), Notes (Text), Other Pages\n",
    "\n",
    "### Preprocessing Pipeline:\n",
    "1. Extract pages from PDFs as images\n",
    "2. Create/load labels\n",
    "3. Train/Val/Test split (stratified)\n",
    "4. Model-specific transformations\n",
    "5. Data augmentation\n",
    "6. Create PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q timm  # For EfficientNet and ViT\n",
    "!pip install -q albumentations  # Advanced augmentations\n",
    "!pip install -q pymupdf Pillow opencv-python-headless\n",
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Callable\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "# timm for modern architectures\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Check device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - UPDATE THESE PATHS\n",
    "# ============================================\n",
    "\n",
    "# Source data path (PDFs)\n",
    "PDF_DATA_PATH = \"/content/drive/MyDrive/YOUR_DATASET_FOLDER\"  # <-- UPDATE THIS\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/FS_Classification\"  # <-- Where to save processed data\n",
    "IMAGES_DIR = os.path.join(OUTPUT_DIR, \"images\")\n",
    "LABELS_PATH = os.path.join(OUTPUT_DIR, \"labels.csv\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'test_size': 0.15,\n",
    "    'val_size': 0.15,  # of remaining after test split\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 2,\n",
    "    'num_classes': 5,\n",
    "    'extraction_dpi': 150,  # DPI for PDF to image conversion\n",
    "}\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = [\n",
    "    'Independent Auditors Report',\n",
    "    'Financial Sheets',\n",
    "    'Notes Tabular',\n",
    "    'Notes Text',\n",
    "    'Other Pages'\n",
    "]\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. PDF to Image Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_pages(pdf_path: str, output_dir: str, dpi: int = 150) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract all pages from a PDF and save as images.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        output_dir: Directory to save images\n",
    "        dpi: Resolution for rendering\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with page info\n",
    "    \"\"\"\n",
    "    pages_info = []\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        zoom = dpi / 72\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            pix = page.get_pixmap(matrix=mat)\n",
    "            \n",
    "            # Create filename\n",
    "            img_filename = f\"{pdf_name}_page_{page_num + 1:04d}.png\"\n",
    "            img_path = os.path.join(output_dir, img_filename)\n",
    "            \n",
    "            # Save image\n",
    "            pix.save(img_path)\n",
    "            \n",
    "            pages_info.append({\n",
    "                'pdf_name': pdf_name,\n",
    "                'page_num': page_num + 1,\n",
    "                'total_pages': len(doc),\n",
    "                'image_path': img_path,\n",
    "                'image_filename': img_filename,\n",
    "                'width': pix.width,\n",
    "                'height': pix.height\n",
    "            })\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "    \n",
    "    return pages_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_pdfs(pdf_dir: str, output_dir: str, dpi: int = 150) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract all pages from all PDFs in a directory.\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs\n",
    "        output_dir: Directory to save images\n",
    "        dpi: Resolution for rendering\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all page information\n",
    "    \"\"\"\n",
    "    # Find all PDFs\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(pdf_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    # Extract all pages\n",
    "    all_pages = []\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Extracting PDFs\"):\n",
    "        pages = extract_pdf_pages(pdf_path, output_dir, dpi)\n",
    "        all_pages.extend(pages)\n",
    "    \n",
    "    df = pd.DataFrame(all_pages)\n",
    "    print(f\"\\nExtracted {len(df)} pages from {len(pdf_files)} PDFs\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if images already extracted\n",
    "existing_images = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.png')] if os.path.exists(IMAGES_DIR) else []\n",
    "\n",
    "if len(existing_images) > 0:\n",
    "    print(f\"Found {len(existing_images)} existing images in {IMAGES_DIR}\")\n",
    "    print(\"Skipping extraction. Delete the images folder to re-extract.\")\n",
    "    \n",
    "    # Reconstruct DataFrame from existing images\n",
    "    pages_data = []\n",
    "    for img_file in existing_images:\n",
    "        parts = img_file.rsplit('_page_', 1)\n",
    "        if len(parts) == 2:\n",
    "            pdf_name = parts[0]\n",
    "            page_num = int(parts[1].split('.')[0])\n",
    "            img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "            \n",
    "            # Get image dimensions\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            img.close()\n",
    "            \n",
    "            pages_data.append({\n",
    "                'pdf_name': pdf_name,\n",
    "                'page_num': page_num,\n",
    "                'image_path': img_path,\n",
    "                'image_filename': img_file,\n",
    "                'width': w,\n",
    "                'height': h\n",
    "            })\n",
    "    \n",
    "    pages_df = pd.DataFrame(pages_data)\n",
    "else:\n",
    "    # Extract all PDFs\n",
    "    print(\"Extracting pages from PDFs...\")\n",
    "    pages_df = extract_all_pdfs(PDF_DATA_PATH, IMAGES_DIR, dpi=CONFIG['extraction_dpi'])\n",
    "\n",
    "print(f\"\\nTotal pages: {len(pages_df)}\")\n",
    "print(f\"Unique PDFs: {pages_df['pdf_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Labeling Strategy\n",
    "\n",
    "Since manual labeling for 1,179 pages can be time-consuming, we provide multiple options:\n",
    "1. **Load existing labels** (if you have them)\n",
    "2. **Use cluster assignments** as pseudo-labels from EDA\n",
    "3. **Semi-supervised approach**: Label a subset and propagate\n",
    "4. **Manual labeling tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTION 1: Load existing labels\n",
    "# ============================================\n",
    "\n",
    "# Uncomment and modify if you have labels\n",
    "# EXISTING_LABELS_PATH = \"/content/drive/MyDrive/your_labels.csv\"\n",
    "# if os.path.exists(EXISTING_LABELS_PATH):\n",
    "#     labels_df = pd.read_csv(EXISTING_LABELS_PATH)\n",
    "#     pages_df = pages_df.merge(labels_df, on=['pdf_name', 'page_num'], how='left')\n",
    "#     print(f\"Loaded labels for {pages_df['label'].notna().sum()} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTION 2: Load cluster assignments from EDA\n",
    "# ============================================\n",
    "\n",
    "EDA_RESULTS_PATH = os.path.join(OUTPUT_DIR, \"page_analysis_results.csv\")\n",
    "\n",
    "if os.path.exists(EDA_RESULTS_PATH):\n",
    "    eda_df = pd.read_csv(EDA_RESULTS_PATH)\n",
    "    if 'cluster' in eda_df.columns:\n",
    "        pages_df = pages_df.merge(\n",
    "            eda_df[['pdf_name', 'page_num', 'cluster']], \n",
    "            on=['pdf_name', 'page_num'], \n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"Loaded cluster assignments for {pages_df['cluster'].notna().sum()} pages\")\n",
    "        print(f\"\\nCluster distribution:\")\n",
    "        print(pages_df['cluster'].value_counts().sort_index())\n",
    "else:\n",
    "    print(f\"EDA results not found at {EDA_RESULTS_PATH}\")\n",
    "    print(\"Run EDA notebook first or create labels manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTION 3: Create label column based on rules or clusters\n",
    "# ============================================\n",
    "\n",
    "# If using clusters as pseudo-labels, map them to class names\n",
    "# IMPORTANT: Review cluster samples in EDA to determine appropriate mapping\n",
    "\n",
    "# Example mapping (ADJUST BASED ON YOUR EDA RESULTS):\n",
    "# From EDA clustering:\n",
    "# Cluster 0: 635 pages (53.9%) - Likely Notes (Text) - largest cluster\n",
    "# Cluster 1: 457 pages (38.8%) - Likely Notes (Tabular) or Financial Sheets\n",
    "# Cluster 2: 62 pages (5.3%)   - Could be Independent Auditor's Report\n",
    "# Cluster 3: 2 pages (0.2%)    - Outliers/Other\n",
    "# Cluster 4: 23 pages (2.0%)   - Likely Other Pages (covers, TOC)\n",
    "\n",
    "CLUSTER_TO_LABEL_MAP = {\n",
    "    0: 'Notes Text',\n",
    "    1: 'Notes Tabular',  # or Financial Sheets - verify with EDA visuals\n",
    "    2: 'Independent Auditors Report',\n",
    "    3: 'Other Pages',\n",
    "    4: 'Other Pages'\n",
    "}\n",
    "\n",
    "if 'cluster' in pages_df.columns:\n",
    "    pages_df['label'] = pages_df['cluster'].map(CLUSTER_TO_LABEL_MAP)\n",
    "    print(\"Label distribution (from cluster mapping):\")\n",
    "    print(pages_df['label'].value_counts())\n",
    "    print(\"\\n⚠️  IMPORTANT: Verify this mapping by reviewing cluster samples in EDA!\")\n",
    "else:\n",
    "    print(\"No cluster column found. Please create labels manually.\")\n",
    "    pages_df['label'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTION 4: Manual labeling helper\n",
    "# ============================================\n",
    "\n",
    "def create_labeling_interface(df: pd.DataFrame, start_idx: int = 0, n_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Display images for manual labeling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with image paths\n",
    "        start_idx: Starting index\n",
    "        n_samples: Number of samples to display\n",
    "    \"\"\"\n",
    "    print(\"Class options:\")\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        print(f\"  {i}: {name}\")\n",
    "    print()\n",
    "    \n",
    "    end_idx = min(start_idx + n_samples, len(df))\n",
    "    \n",
    "    n_cols = 2\n",
    "    n_rows = (end_idx - start_idx + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 6*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for i, idx in enumerate(range(start_idx, end_idx)):\n",
    "        row = df.iloc[idx]\n",
    "        img = Image.open(row['image_path'])\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        current_label = row.get('label', 'Unlabeled')\n",
    "        axes[i].set_title(f\"Index: {idx}\\n{row['pdf_name']}\\nPage: {row['page_num']}\\nCurrent: {current_label}\", fontsize=9)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    for i in range(end_idx - start_idx, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to use manual labeling\n",
    "# create_labeling_interface(pages_df, start_idx=0, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual label assignment (if needed)\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# Example: Set specific labels\n",
    "# pages_df.loc[pages_df.index == 0, 'label'] = 'Other Pages'  # First page is cover\n",
    "# pages_df.loc[pages_df.index == 1, 'label'] = 'Other Pages'  # TOC\n",
    "\n",
    "# Or batch assign based on patterns\n",
    "# pages_df.loc[pages_df['page_num'] == 1, 'label'] = 'Other Pages'  # All first pages are covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify labels\n",
    "print(\"=\"*50)\n",
    "print(\"LABEL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'label' in pages_df.columns and pages_df['label'].notna().any():\n",
    "    print(f\"\\nLabeled pages: {pages_df['label'].notna().sum()} / {len(pages_df)}\")\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    label_counts = pages_df['label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  {label}: {count} ({count/len(pages_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    label_counts.plot(kind='bar', color=sns.color_palette('husl', len(label_counts)))\n",
    "    plt.title('Label Distribution', fontweight='bold')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No labels found. Please create labels using one of the options above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "if 'label' in pages_df.columns and pages_df['label'].notna().any():\n",
    "    # Only use labeled data\n",
    "    labeled_df = pages_df[pages_df['label'].notna()].copy()\n",
    "    \n",
    "    # Create label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(CLASS_NAMES)\n",
    "    \n",
    "    labeled_df['label_encoded'] = label_encoder.transform(labeled_df['label'])\n",
    "    \n",
    "    print(f\"Label encoding:\")\n",
    "    for i, name in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {i}: {name}\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    with open(os.path.join(OUTPUT_DIR, 'label_encoder.pkl'), 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "else:\n",
    "    labeled_df = pages_df.copy()\n",
    "    print(\"WARNING: No labels available. Models will need labels for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_split(df: pd.DataFrame, test_size: float = 0.15, \n",
    "                            val_size: float = 0.15, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create stratified train/val/test splits.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'label_encoded' column\n",
    "        test_size: Proportion for test set\n",
    "        val_size: Proportion for validation set (from remaining after test)\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    # First split: train+val vs test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        stratify=df['label_encoded'],\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    actual_val_size = val_size / (1 - test_size)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=actual_val_size,\n",
    "        stratify=train_val_df['label_encoded'],\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "if 'label_encoded' in labeled_df.columns:\n",
    "    train_df, val_df, test_df = create_stratified_split(\n",
    "        labeled_df, \n",
    "        test_size=CONFIG['test_size'],\n",
    "        val_size=CONFIG['val_size'],\n",
    "        seed=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset splits:\")\n",
    "    print(f\"  Train: {len(train_df)} ({len(train_df)/len(labeled_df)*100:.1f}%)\")\n",
    "    print(f\"  Val:   {len(val_df)} ({len(val_df)/len(labeled_df)*100:.1f}%)\")\n",
    "    print(f\"  Test:  {len(test_df)} ({len(test_df)/len(labeled_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Save splits\n",
    "    train_df.to_csv(os.path.join(OUTPUT_DIR, 'train.csv'), index=False)\n",
    "    val_df.to_csv(os.path.join(OUTPUT_DIR, 'val.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(OUTPUT_DIR, 'test.csv'), index=False)\n",
    "    print(f\"\\nSplits saved to {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"Cannot create splits without labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification\n",
    "if 'label_encoded' in labeled_df.columns:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for ax, (name, df) in zip(axes, [('Train', train_df), ('Val', val_df), ('Test', test_df)]):\n",
    "        counts = df['label'].value_counts()\n",
    "        ax.bar(range(len(counts)), counts.values, color=sns.color_palette('husl', len(counts)))\n",
    "        ax.set_xticks(range(len(counts)))\n",
    "        ax.set_xticklabels(counts.index, rotation=45, ha='right')\n",
    "        ax.set_title(f'{name} Set Distribution\\n(n={len(df)})', fontweight='bold')\n",
    "        ax.set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model-Specific Preprocessing\n",
    "\n",
    "Each model requires different input sizes and normalization:\n",
    "\n",
    "| Model | Input Size | Normalization |\n",
    "|-------|------------|---------------|\n",
    "| ResNet50 | 224x224 | ImageNet mean/std |\n",
    "| EfficientNet-B2 | 260x260 | ImageNet mean/std |\n",
    "| ViT-Base | 224x224 | ImageNet mean/std |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization values\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Model-specific configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'resnet50': {\n",
    "        'input_size': 224,\n",
    "        'mean': IMAGENET_MEAN,\n",
    "        'std': IMAGENET_STD,\n",
    "        'timm_name': 'resnet50',\n",
    "        'pretrained': True\n",
    "    },\n",
    "    'efficientnet_b2': {\n",
    "        'input_size': 260,\n",
    "        'mean': IMAGENET_MEAN,\n",
    "        'std': IMAGENET_STD,\n",
    "        'timm_name': 'efficientnet_b2',\n",
    "        'pretrained': True\n",
    "    },\n",
    "    'vit_base': {\n",
    "        'input_size': 224,\n",
    "        'mean': IMAGENET_MEAN,\n",
    "        'std': IMAGENET_STD,\n",
    "        'timm_name': 'vit_base_patch16_224',\n",
    "        'pretrained': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model configurations:\")\n",
    "for name, config in MODEL_CONFIGS.items():\n",
    "    print(f\"  {name}: input_size={config['input_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Augmentation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(input_size: int, mean: List[float], std: List[float]) -> A.Compose:\n",
    "    \"\"\"\n",
    "    Get training augmentation pipeline using Albumentations.\n",
    "    \n",
    "    Augmentations suitable for document images:\n",
    "    - Geometric: small rotations, slight perspective changes\n",
    "    - Quality: compression artifacts, slight blur\n",
    "    - No color jittering (documents are mostly grayscale/B&W)\n",
    "    \n",
    "    Args:\n",
    "        input_size: Target image size\n",
    "        mean: Normalization mean\n",
    "        std: Normalization std\n",
    "    \n",
    "    Returns:\n",
    "        Albumentations Compose transform\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Resize with aspect ratio preservation, then pad/crop\n",
    "        A.LongestMaxSize(max_size=int(input_size * 1.1)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(input_size * 1.1), \n",
    "            min_width=int(input_size * 1.1),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=(255, 255, 255)  # White padding for documents\n",
    "        ),\n",
    "        A.RandomCrop(height=input_size, width=input_size),\n",
    "        \n",
    "        # Geometric augmentations (subtle for documents)\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=5,  # Small rotation for documents\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=(255, 255, 255),\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.Perspective(\n",
    "            scale=(0.02, 0.05),  # Subtle perspective change\n",
    "            p=0.3\n",
    "        ),\n",
    "        \n",
    "        # Quality augmentations\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "            A.MotionBlur(blur_limit=3, p=1.0),\n",
    "        ], p=0.2),\n",
    "        \n",
    "        A.ImageCompression(quality_lower=75, quality_upper=100, p=0.3),\n",
    "        \n",
    "        # Noise\n",
    "        A.GaussNoise(var_limit=(5.0, 20.0), p=0.2),\n",
    "        \n",
    "        # Slight contrast variation\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.1,\n",
    "            contrast_limit=0.1,\n",
    "            p=0.3\n",
    "        ),\n",
    "        \n",
    "        # Horizontal flip (may not be suitable for all document types)\n",
    "        # A.HorizontalFlip(p=0.5),  # Uncomment if appropriate\n",
    "        \n",
    "        # Normalize and convert to tensor\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_transforms(input_size: int, mean: List[float], std: List[float]) -> A.Compose:\n",
    "    \"\"\"\n",
    "    Get validation/test transform pipeline.\n",
    "    Only resize and normalize - no augmentation.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Target image size\n",
    "        mean: Normalization mean\n",
    "        std: Normalization std\n",
    "    \n",
    "    Returns:\n",
    "        Albumentations Compose transform\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Resize maintaining aspect ratio, then center crop\n",
    "        A.LongestMaxSize(max_size=input_size),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=input_size, \n",
    "            min_width=input_size,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=(255, 255, 255)\n",
    "        ),\n",
    "        A.CenterCrop(height=input_size, width=input_size),\n",
    "        \n",
    "        # Normalize and convert to tensor\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Simple torchvision transforms (for reference)\n",
    "\n",
    "def get_torchvision_train_transforms(input_size: int, mean: List[float], std: List[float]) -> T.Compose:\n",
    "    \"\"\"Torchvision-based training transforms.\"\"\"\n",
    "    return T.Compose([\n",
    "        T.Resize((int(input_size * 1.1), int(input_size * 1.1))),\n",
    "        T.RandomCrop(input_size),\n",
    "        T.RandomRotation(degrees=5),\n",
    "        T.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "def get_torchvision_val_transforms(input_size: int, mean: List[float], std: List[float]) -> T.Compose:\n",
    "    \"\"\"Torchvision-based validation transforms.\"\"\"\n",
    "    return T.Compose([\n",
    "        T.Resize((input_size, input_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentations\n",
    "def visualize_augmentations(image_path: str, transform: A.Compose, n_samples: int = 6):\n",
    "    \"\"\"\n",
    "    Visualize multiple augmented versions of an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to source image\n",
    "        transform: Augmentation pipeline\n",
    "        n_samples: Number of augmented versions to show\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, (n_samples + 2) // 2, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Original', fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Augmented versions (unnormalize for visualization)\n",
    "    for i in range(1, n_samples + 1):\n",
    "        augmented = transform(image=img)['image']\n",
    "        \n",
    "        # Unnormalize for display\n",
    "        aug_img = augmented.permute(1, 2, 0).numpy()\n",
    "        aug_img = aug_img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
    "        aug_img = np.clip(aug_img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(aug_img)\n",
    "        axes[i].set_title(f'Augmented {i}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate augmentations on a sample image\n",
    "if len(train_df) > 0:\n",
    "    sample_image_path = train_df.iloc[0]['image_path']\n",
    "    sample_transform = get_train_transforms(224, IMAGENET_MEAN, IMAGENET_STD)\n",
    "    \n",
    "    print(f\"Visualizing augmentations on: {sample_image_path}\")\n",
    "    visualize_augmentations(sample_image_path, sample_transform, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialStatementDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Financial Statement page classification.\n",
    "    \n",
    "    Supports both Albumentations and torchvision transforms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dataframe: pd.DataFrame,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 use_albumentations: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame with 'image_path' and 'label_encoded' columns\n",
    "            transform: Transform pipeline\n",
    "            use_albumentations: Whether transform is from Albumentations (vs torchvision)\n",
    "        \"\"\"\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        if self.use_albumentations:\n",
    "            image = cv2.imread(row['image_path'])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            image = Image.open(row['image_path']).convert('RGB')\n",
    "        \n",
    "        # Get label\n",
    "        label = row['label_encoded']\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_labels(self) -> np.ndarray:\n",
    "        \"\"\"Return all labels (for weighted sampler).\"\"\"\n",
    "        return self.df['label_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialStatementInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for inference (no labels required).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 image_paths: List[str],\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 use_albumentations: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of image file paths\n",
    "            transform: Transform pipeline\n",
    "            use_albumentations: Whether transform is from Albumentations\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        if self.use_albumentations:\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        \n",
    "        return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. DataLoader Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_sampler(dataset: FinancialStatementDataset) -> WeightedRandomSampler:\n",
    "    \"\"\"\n",
    "    Create weighted random sampler for handling class imbalance.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Training dataset\n",
    "    \n",
    "    Returns:\n",
    "        WeightedRandomSampler\n",
    "    \"\"\"\n",
    "    labels = dataset.get_labels()\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Calculate weights (inverse frequency)\n",
    "    total_samples = len(labels)\n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Assign weight to each sample\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_df: pd.DataFrame, \n",
    "                       val_df: pd.DataFrame, \n",
    "                       test_df: pd.DataFrame,\n",
    "                       model_name: str,\n",
    "                       batch_size: int = 16,\n",
    "                       num_workers: int = 2,\n",
    "                       use_weighted_sampling: bool = True) -> Dict[str, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create DataLoaders for a specific model.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        val_df: Validation DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        model_name: Name of the model (resnet50, efficientnet_b2, vit_base)\n",
    "        batch_size: Batch size\n",
    "        num_workers: Number of data loading workers\n",
    "        use_weighted_sampling: Whether to use weighted sampling for imbalanced classes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train, val, test DataLoaders\n",
    "    \"\"\"\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    input_size = config['input_size']\n",
    "    mean = config['mean']\n",
    "    std = config['std']\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = get_train_transforms(input_size, mean, std)\n",
    "    val_transform = get_val_transforms(input_size, mean, std)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FinancialStatementDataset(train_df, train_transform, use_albumentations=True)\n",
    "    val_dataset = FinancialStatementDataset(val_df, val_transform, use_albumentations=True)\n",
    "    test_dataset = FinancialStatementDataset(test_df, val_transform, use_albumentations=True)\n",
    "    \n",
    "    # Create sampler for training if needed\n",
    "    train_sampler = None\n",
    "    shuffle_train = True\n",
    "    \n",
    "    if use_weighted_sampling:\n",
    "        train_sampler = get_weighted_sampler(train_dataset)\n",
    "        shuffle_train = False  # Sampler handles shuffling\n",
    "    \n",
    "    # Create dataloaders\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_train,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        ),\n",
    "        'val': DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        ),\n",
    "        'test': DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nDataLoaders created for {model_name}:\")\n",
    "    print(f\"  Input size: {input_size}x{input_size}\")\n",
    "    print(f\"  Train batches: {len(dataloaders['train'])}\")\n",
    "    print(f\"  Val batches: {len(dataloaders['val'])}\")\n",
    "    print(f\"  Test batches: {len(dataloaders['test'])}\")\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for all three models\n",
    "all_dataloaders = {}\n",
    "\n",
    "if 'label_encoded' in train_df.columns:\n",
    "    for model_name in MODEL_CONFIGS.keys():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Creating dataloaders for: {model_name.upper()}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        all_dataloaders[model_name] = create_dataloaders(\n",
    "            train_df, val_df, test_df,\n",
    "            model_name=model_name,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            use_weighted_sampling=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Verify DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(dataloader: DataLoader, model_name: str, n_samples: int = 8):\n",
    "    \"\"\"\n",
    "    Visualize a batch from the dataloader.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: DataLoader to visualize\n",
    "        model_name: Name of the model\n",
    "        n_samples: Number of samples to show\n",
    "    \"\"\"\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    mean = np.array(config['mean'])\n",
    "    std = np.array(config['std'])\n",
    "    \n",
    "    # Get a batch\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    n_samples = min(n_samples, len(images))\n",
    "    n_cols = 4\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        img = img * std + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {CLASS_NAMES[labels[i]]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    for i in range(n_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model_name.upper()} - Training Batch Sample', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batches for each model\n",
    "if all_dataloaders:\n",
    "    for model_name, dataloaders in all_dataloaders.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Sample batch for: {model_name.upper()}\")\n",
    "        print('='*50)\n",
    "        visualize_batch(dataloaders['train'], model_name, n_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Model Architecture Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name: str, num_classes: int, pretrained: bool = True) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create a model with pretrained weights.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        num_classes: Number of output classes\n",
    "        pretrained: Whether to use pretrained weights\n",
    "    \n",
    "    Returns:\n",
    "        PyTorch model\n",
    "    \"\"\"\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    timm_name = config['timm_name']\n",
    "    \n",
    "    model = timm.create_model(\n",
    "        timm_name,\n",
    "        pretrained=pretrained,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview model architectures\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name in MODEL_CONFIGS.keys():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    model = create_model(model_name, CONFIG['num_classes'], pretrained=False)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Input size: {MODEL_CONFIGS[model_name]['input_size']}x{MODEL_CONFIGS[model_name]['input_size']}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    dummy_input = torch.randn(1, 3, config['input_size'], config['input_size'])\n",
    "    output = model(dummy_input)\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    \n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Class Weights for Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(train_df: pd.DataFrame, num_classes: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute class weights for handling imbalanced dataset.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame with 'label_encoded' column\n",
    "        num_classes: Number of classes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of class weights\n",
    "    \"\"\"\n",
    "    class_counts = train_df['label_encoded'].value_counts().sort_index()\n",
    "    total_samples = len(train_df)\n",
    "    \n",
    "    # Compute weights using inverse frequency\n",
    "    weights = total_samples / (num_classes * class_counts.values)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "    \n",
    "    return torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display class weights\n",
    "if 'label_encoded' in train_df.columns:\n",
    "    class_weights = compute_class_weights(train_df, CONFIG['num_classes'])\n",
    "    \n",
    "    print(\"Class weights for loss function:\")\n",
    "    for i, (name, weight) in enumerate(zip(CLASS_NAMES, class_weights)):\n",
    "        print(f\"  {i}: {name}: {weight:.4f}\")\n",
    "    \n",
    "    # Save class weights\n",
    "    torch.save(class_weights, os.path.join(OUTPUT_DIR, 'class_weights.pt'))\n",
    "    print(f\"\\nClass weights saved to {OUTPUT_DIR}/class_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Save Preprocessing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all configuration and metadata\n",
    "preprocessing_config = {\n",
    "    'config': CONFIG,\n",
    "    'model_configs': MODEL_CONFIGS,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'imagenet_mean': IMAGENET_MEAN,\n",
    "    'imagenet_std': IMAGENET_STD,\n",
    "    'dataset_stats': {\n",
    "        'total_pages': len(labeled_df) if 'labeled_df' in dir() else 0,\n",
    "        'train_size': len(train_df) if 'train_df' in dir() else 0,\n",
    "        'val_size': len(val_df) if 'val_df' in dir() else 0,\n",
    "        'test_size': len(test_df) if 'test_df' in dir() else 0,\n",
    "    },\n",
    "    'cluster_to_label_map': CLUSTER_TO_LABEL_MAP if 'CLUSTER_TO_LABEL_MAP' in dir() else None\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "config_path = os.path.join(OUTPUT_DIR, 'preprocessing_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(preprocessing_config, f, indent=2)\n",
    "\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*70)\n",
    "print(\"                    PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📁 OUTPUT DIRECTORY: {OUTPUT_DIR}\")\n",
    "print(f\"\\n📊 DATASET:\")\n",
    "print(f\"   Total pages: {len(labeled_df) if 'labeled_df' in dir() else 'N/A'}\")\n",
    "print(f\"   Train: {len(train_df) if 'train_df' in dir() else 'N/A'}\")\n",
    "print(f\"   Val: {len(val_df) if 'val_df' in dir() else 'N/A'}\")\n",
    "print(f\"   Test: {len(test_df) if 'test_df' in dir() else 'N/A'}\")\n",
    "\n",
    "print(f\"\\n🏷️  CLASSES: {CONFIG['num_classes']}\")\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    print(f\"   {i}: {name}\")\n",
    "\n",
    "print(f\"\\n🤖 MODELS PREPARED:\")\n",
    "for model_name, config in MODEL_CONFIGS.items():\n",
    "    print(f\"   - {model_name}: input={config['input_size']}x{config['input_size']}\")\n",
    "\n",
    "print(f\"\\n📦 FILES SAVED:\")\n",
    "saved_files = [\n",
    "    'train.csv', 'val.csv', 'test.csv',\n",
    "    'label_encoder.pkl', 'class_weights.pt',\n",
    "    'preprocessing_config.json'\n",
    "]\n",
    "for f in saved_files:\n",
    "    path = os.path.join(OUTPUT_DIR, f)\n",
    "    exists = '✓' if os.path.exists(path) else '✗'\n",
    "    print(f\"   {exists} {f}\")\n",
    "\n",
    "print(f\"\\n✅ Preprocessing complete! Ready for model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "The data is now preprocessed and ready for training. The next notebook should:\n",
    "\n",
    "1. **Load preprocessed data** using the saved CSV files and configurations\n",
    "2. **Train three models**:\n",
    "   - ResNet50 with transfer learning\n",
    "   - EfficientNet-B2 with transfer learning\n",
    "   - Vision Transformer (ViT-Base)\n",
    "3. **Evaluate and compare** model performance\n",
    "4. **Save best model** for inference\n",
    "\n",
    "### Code to Load Data in Training Notebook:\n",
    "\n",
    "```python\n",
    "# Load configuration\n",
    "with open('preprocessing_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load data splits\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Load label encoder\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Load class weights\n",
    "class_weights = torch.load('class_weights.pt')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
